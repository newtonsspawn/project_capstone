{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Technical Report - Kaggle's Freesound Audio Tagging Competition\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Audio signal processing is a very complex field with many applications, ranging\n",
    "from automated voice detection (Siri) to music recognition (Shazam). Freesound\n",
    "has launched a competition, using the Kaggle platform, to build a model that can\n",
    "train on both clean and noisy sound samples to be able to predict and classify\n",
    "what new sounds it reads may be.\n",
    "\n",
    "The challenge here will be figuring out how best to transform incoming audio\n",
    "signals and how best to model on those transformations to best make these\n",
    "predictions.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "The data was collected directly from \n",
    "[Kaggle](https://www.kaggle.com/c/freesound-audio-tagging-2019/data) using their\n",
    "API and downloaded to an AWS EFS instance. This EFS instance was mounted to a\n",
    "t2.micro AWS EC2 instance where all of the processing was completed.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "The dataset contained three sets of sound files, broken into a curated set, a \n",
    "noisy set, and a sample set. Each of the sound files range from about a third of \n",
    "a second to thirty seconds in length.\n",
    " \n",
    "The curated set contained 4,970 sound clips. The noisy set contained 19,815\n",
    "sound clips. Each of these sets were labelled in an included file in .csv\n",
    "format. I chose not to deal with the sample set of files as these were not\n",
    "labelled and as such, were not helpful for my analysis.\n",
    "\n",
    "The labels cover a broad array of different categories of sounds, including\n",
    "voices, knocking, typing, purring, barking, farting, rain, glass shattering, \n",
    "musical instruments, bells, etc. There are 80 types of sounds in total. The \n",
    "majority of the sounds are multi-labelled, and the noisy dataset includes \n",
    "background noises as well.  \n",
    "\n",
    "### Comparing Wave Signals\n",
    "\n",
    "To look at the wave files graphically, I used the *wave* library. This library\n",
    "allowed me to read in the signals' amplitudes and enumerate them according to \n",
    "their frame rates for plotting purposes.\n",
    "\n",
    "Here is an example of a fart sound from the curated set. It only includes the\n",
    "fart noise without any background noises and thus provides a clean signal to \n",
    "process.\n",
    "\n",
    "![](./images/wave_plots/1a94671f_Fart_(curated).png)\n",
    "\n",
    "\n",
    "The next two examples show different examples from the noisy set. As you can\n",
    "see, the background noise makes it very hard to isolate the signal and make any\n",
    "sense of the sound graphically. The first one is an example of a motorcycle\n",
    "and the second of a simple raindrop.\n",
    "\n",
    "![](./images/wave_plots/000b6cfb_Motorcycle_(noisy).png)\n",
    "\n",
    "![](./images/wave_plots/0019adae_Raindrop_(noisy).png)\n",
    "\n",
    "It is clear that just looking at the amplitudes of the wave signals for each of\n",
    "the sounds will not be enough to distinguish between similar sounds, especially\n",
    "those with background noise in the file.\n",
    "\n",
    "### Other Sound-Related Parameters\n",
    "\n",
    "I also looked at the parameters of the wave files to get a sense of the types\n",
    "of information I could gain from each sample. All of the sounds have a single\n",
    "channel and a framerate of 44100. The number of frames vary based on the length\n",
    "of the file and none of them are compressed.\n",
    "\n",
    "###### code sample\n",
    "```python\n",
    "f = wave.open('../data/train_curated/0019ef41.wav')\n",
    "print(f.getparams())\n",
    "f.close()\n",
    "```\n",
    "###### output\n",
    "```python\n",
    "_wave_params(nchannels=1, sampwidth=2, framerate=44100, nframes=90616, comptype='NONE', compname='not compressed')\n",
    "```\n",
    "\n",
    "Based on this information, I decided a more complex breakdown of the sound\n",
    "signals would be necessary to conduct any meaningful analysis. \n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "### Sound Samples\n",
    "\n",
    "I processed the sounds samples for modeling using three different methods:\n",
    "1. amplitudes from the raw signals,\n",
    "2. Fourier transformations to decompose the signals into their constituent frequencies, and\n",
    "3. Mel-frequency cepstral coefficients (MFCCs).\n",
    "\n",
    "The first two methods did not offer much in help in terms of a strong variance\n",
    "between varying sound signals. Hence, I decided to pre-process my samples using\n",
    "MFCCs for modeling.\n",
    "\n",
    "MFCCs are derived as follows (via [Wikipedia](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)):\n",
    "1. Take the Fourier transform of (a windowed excerpt of) a signal.\n",
    "2. Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\n",
    "3. Take the logs of the powers at each of the mel frequencies.\n",
    "4. Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "5. The MFCCs are the amplitudes of the resulting spectrum.\n",
    "\n",
    "The MFCCs essentially represent the short-term power spectrum of the sound \n",
    "signal. The MFCCs for each sound sample showed a significant variance between\n",
    "samples and thus were better suited for modeling purposes.\n",
    "\n",
    "To perform the transformations, I relied on the *librosa* library. I processed\n",
    "the entire dataset, curated and noisy, parsing each with 20, 40, and 60 MFCCs.\n",
    "\n",
    "### Labels\n",
    "\n",
    "The labels were not initially given in a user-friendly format. They were given\n",
    "as a single column in the CSV file, with commas separating multi-labelled\n",
    "samples. I converted these labels to a sparse matrix, consisting of 80 columns\n",
    "of 0s and 1s, with multiple 1s representing multi-labelled sound files.\n",
    "\n",
    "## Modeling\n",
    "\n",
    "I opted to use a Sequential Neural Network (SNN) to perform the modeling on the\n",
    "transformed sound files. I conducted all of the modeling on an t2.micro AWS EC2 \n",
    "instance with an EFS instance mounted to it.\n",
    "\n",
    "### Strategy\n",
    "\n",
    "#### Single-Labelled Modeling\n",
    "\n",
    "I conducted the first models with the labels in their original form, treating\n",
    "the comma-separated multi-labels as a single label. I removed samples without\n",
    "duplicate labels and ran them through an SNN to see how well they performed this\n",
    "way. Yes, this was cheating, as it was not returning individual sound \n",
    "predictions and instead just groups of sounds but I thought it was worth seeing\n",
    "the results.\n",
    "\n",
    "After lots of trial and error and fine-tuning, this is the SNN that yielded the\n",
    "best results:\n",
    "```python\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_shape=(60,)))\n",
    "\n",
    "model.add(Dense(256, activation='relu',))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu',))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu',))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu',))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')\n",
    "```\n",
    "\n",
    "I ran this SNN with all of my MFCC transformations, using 20, 40, and 60 MFCCs.\n",
    "The best performing transformation by far was the one with 40 MFCCs. Here are\n",
    "the graphs showing the accuracy and loss metrics over each training epoch.\n",
    "\n",
    "![](./images/NN_metrics/curated_NN_40_acc_plot.png)\n",
    "\n",
    "![](./images/NN_metrics/curated_NN_40_loss_plot.png)\n",
    "\n",
    "The results were not bad! I got to about 22% accuracy before the model started\n",
    "overfitting. But remember, we are cheating here. I expected the model to perform \n",
    "much worse once we start using the multi-labelled sparse matrix.\n",
    "\n",
    "#### Multi-Labelled Modeling\n",
    "\n",
    "The next step was to conduct the modeling using the multi-labelled sparse\n",
    "matrix. I had to make a few changes to the SNN such as the activation function\n",
    "for the output layer and the associated loss function, as I'd be returning a \n",
    "a binary 1 or 0 for each label in the resulting matrix.\n",
    "```python\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_shape=(40,)))\n",
    "\n",
    "model.add(Dense(256, activation='relu',))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu',))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu',))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu',))\n",
    "\n",
    "model.add(Dense(num_labels, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='sigmoid',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')\n",
    "```\n",
    "\n",
    "This did not go well at all. The loss function could not measure the model's\n",
    "performance correctly because with a sparse matrix containing 80 labels, there'd\n",
    "always be at least 78 true negatives (TNs), making it over 98% accurate even\n",
    "with a random guess. the resulting graphs so just how hard of a time the model\n",
    "had.\n",
    "\n",
    "![](./images/NN_metrics/NN_multi_40_acc_plot.png)\n",
    "\n",
    "![](./images/NN_metrics/NN_multi_40_loss_plot.png)\n",
    "\n",
    "#### Customized Multi-Labelled Modeling\n",
    "\n",
    "The next step was to customize the previous SNN to allow the model to train on\n",
    "each label independently, using accuracy for each, and using precision to \n",
    "determine its overall success instead of accuracy, since precision does not take\n",
    "into account TNs.\n",
    "\n",
    "As this will be my production model for the time being, the results are\n",
    "discussed in the following 'Results' section. \n",
    "\n",
    "### Results\n",
    "\n",
    "After all of the modeling with the various transformations, here is a summary\n",
    "of the useful metrics.\n",
    "\n",
    "| Model                 | Accuracy  | Precision |\n",
    "| --------------------- |:---------:| ---------:|\n",
    "| baseline              | .002      | .002      |\n",
    "| SNN (single-labelled) | .224      | n/a       |\n",
    "| SNN (multi-labelled)  | n/a       | .003      |\n",
    "| SNN (custom)          | n/a       | .055      | \n",
    "\n",
    "While 5.5% precision is not a result worth bragging about, it is 27.5 times\n",
    "better than the baseline. This means we are at least moving in the right\n",
    "direction.\n",
    "\n",
    "Finally, I wanted to see which sounds preformed the best in my custom \n",
    "multi-labelled model and which performed the worst.\n",
    "\n",
    "The top 10 sounds my model was able to predict are in the plot below.\n",
    "\n",
    "![](./images/best_performers_plot.png)\n",
    "\n",
    "The worst 10 sounds my model was able to predict are in the plot below.\n",
    "\n",
    "![](./images/worst_performers_plot.png)\n",
    "\n",
    "The top and worst performing sounds seem to make sense. The best performing are\n",
    "very distinct sounds while the worst performing have many competing sounds that\n",
    "are very similar in nature, such as determining the difference between a woman's \n",
    "and a child's voice.\n",
    "\n",
    "## Future Steps\n",
    "\n",
    "1. **Improve Loss Function** - There is reason to believe that a better\n",
    "performing loss function will result in the model performing better on each\n",
    "successive run.\n",
    "\n",
    "2. **Different Transformations** - There are many more ways to transform sounds\n",
    "and conduct signal processing. We could apply different techniques here and even\n",
    "combine different types to see if they perform better in our models.\n",
    "\n",
    "3. **Background Noise Reducer** - Because the model has a hard time separating\n",
    "the sounds from the background noise in the noisy samples, developing a method\n",
    "to subtract the background noise from each file would serve to help isolate the\n",
    "target sounds better.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}